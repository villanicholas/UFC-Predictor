{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"out.csv\").dropna()\n",
    "\n",
    "# Modify the 'winner' column based on conditions\n",
    "data['winner'] = data.apply(lambda row: 1 if row['winner'] == row['fighter_1'] else 0, axis=1)\n",
    "\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['fight_id', 'fighter_1_id', 'fighter_1', 'fighter_2', 'fighter_2_id','decision_method','fight_duration_lastrnd','time_format','date', 'location','height_x','name_x','weight_x','height_y','name_y','weight_y' ]\n",
    "data_cleaned = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "data_cleaned.head()\n",
    "\n",
    "data_cleaned.to_csv(\"cleanedData2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split up the data\n",
    "\n",
    "winners = data_cleaned.iloc[:,0]\n",
    "features = data_cleaned.iloc[:,1]\n",
    "\n",
    "\n",
    "features = features.sample(frac =1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"encoded_cleanedDataNEW.csv\") # original dataset\n",
    "##print(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "'''keep = np.array([4,2,1,6,10,11,12,37,38,63,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,104,105,130,132,133,134,135,136,137,138,139,140,141,142,143,144,145]) - 1\n",
    "df = df.iloc[:,:] # interesting fields\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"]) # date as datetime\n",
    "\n",
    "\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True) # replace empty string with nan\n",
    "df = df.fillna(np.nan) # Fill empty and NaNs values with NaN\n",
    "\n",
    "df = df.round(3) # format data\n",
    "\n",
    "df.rename(columns={ # some renaming\n",
    "    \"B_win_by_KO/TKO\": \"B_win_by_KO_TKO\",\n",
    "    \"R_win_by_KO/TKO\": \"R_win_by_KO_TKO\"},inplace=True)\n",
    "##print(f\"shape: {df.shape}\")\n",
    "\n",
    "# considering matches with a winner as this will be a binary classification\n",
    "drawRate = round(len(df[\"Winner\"][df[\"Winner\"]== \"Draw\"])/len(df.index)*100,3)\n",
    "print(f\"Draw rate is: {drawRate}%, converting to nan\")\n",
    "df[\"Winner\"] = df[\"Winner\"].replace(\"Draw\", np.nan) # convert to nan to be dropped later\n",
    "##df.dropna()'''\n",
    "\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##shuffling the data \n",
    "df_shuffled = df.sample(frac = 1)\n",
    "df_shuffled.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##split up the data \n",
    "df_shuffled.columns\n",
    "\n",
    "'''x = df_shuffled.drop([], axis = 1).values\n",
    "y = df_shuffled[\"Winner\"].values'''\n",
    "y = df_shuffled.iloc[:,0]\n",
    "x = df_shuffled.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomMinMaxScaler:\n",
    "    def __init__(self):\n",
    "        self.min_ = None\n",
    "        self.max_ = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        # Compute min and max values for each feature\n",
    "        self.min_ = np.min(data, axis=0)\n",
    "        self.max_ = np.max(data, axis=0)\n",
    "\n",
    "    def transform(self, data):\n",
    "        # Scale data to the range [0, 1]\n",
    "        return (data - self.min_) / (self.max_ - self.min_)\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y,test_size = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''winners = y = df_shuffled[\"Winner\"].values'''\n",
    "\n",
    "scaler = CustomMinMaxScaler()\n",
    "\n",
    "xTrainScaled = scaler.fit_transform(xTrain)\n",
    "xTestScaled = scaler.fit_transform(xTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model1 = LogisticRegression(penalty = 'l1', tol = 1, solver = 'liblinear',\n",
    "                            multi_class = \"auto\",\n",
    "                            fit_intercept = False, \n",
    "                            max_iter =3)\n",
    "model1.fit(xTrainScaled, yTrain)\n",
    "joblib.dump(model1, 'model1.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model = joblib.load(\"model1.joblib\")\n",
    "prediction = testing_model.predict(xTestScaled)\n",
    "print(prediction)\n",
    "for i in prediction:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score for Logistic Regression Model: %s\" % (sklearn.metrics.accuracy_score(yTest,prediction)))\n",
    "print(\"Precision score for logistic Regression Model: %s\" % (sklearn.metrics.precision_score(yTest,prediction)))\n",
    "print(\"Recall score for Logistic Regression Model: %s\" % (sklearn.metrics.recall_score(yTest,prediction)))\n",
    "print(\"F1 score for Logistic Regression Model: %s\" % (sklearn.metrics.f1_score(yTest,prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model2 = KNeighborsClassifier(n_neighbors = 4, p =3)\n",
    "model2.fit(xTrainScaled,yTrain)\n",
    "joblib.dump(model2, 'model2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnModel = joblib.load(\"model2.joblib\")\n",
    "knnPrediction = knnModel.predict(xTestScaled)\n",
    "for i in range(len(knnPrediction)): \n",
    "    print(\"Model 2 prediction based on data B for item: %d : %s\" % (i + 1, knnPrediction[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnModel_c = joblib.load(\"model2.joblib\")\n",
    "knnPrediction_c = knnModel_c.predict(xTestScaled)\n",
    "\n",
    "C_answers = yTest\n",
    "TP, FP = 0,0\n",
    "#for i in range(len(knnPrediction_c)): \n",
    "    #print(\"Model 2 prediction based on data C: %s\" % (knnPrediction_c[i]))\n",
    "print(\"Accuracy Score for KNN Model: %s\" %(sklearn.metrics.accuracy_score(C_answers,knnPrediction_c)))\n",
    "print(\"Precision score for KNN Model: %s\" %(sklearn.metrics.precision_score(C_answers,knnPrediction_c)))\n",
    "print(\"Recall Score for KNN Model: %s\" % (sklearn.metrics.recall_score(C_answers,knnPrediction_c)))\n",
    "print(\"F1 Score for KNN Model: %s\" % (sklearn.metrics.f1_score(C_answers,knnPrediction_c)))\n",
    "FDR_knn = 1 - sklearn.metrics.precision_score(C_answers, knnPrediction_c)\n",
    "print(\"False Detection Rate for KNN Model: %s\" %(FDR_knn))\n",
    "print(\"Matthews Coeffiecen for KNN Model: %s\" % (sklearn.metrics.matthews_corrcoef(C_answers,knnPrediction_c)))\n",
    "\n",
    "print(f'=====================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model3 = MLPClassifier(solver = \"lbfgs\", alpha=.1, hidden_layer_sizes=(1,5))\n",
    "model3.fit(xTrainScaled,yTrain)\n",
    "joblib.dump(model3, 'model3.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpModel = joblib.load(\"model3.joblib\")\n",
    "mlpPrediction = mlpModel.predict(xTestScaled)\n",
    "for i in range(len(mlpPrediction)): \n",
    "    print(\"Model 4 prediction for item: %d : %s\" % (i + 1, mlpPrediction[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rand forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model4 = RandomForestClassifier(max_depth = 2)\n",
    "model4.fit(xTrainScaled,yTrain)\n",
    "joblib.dump(model4,'model4.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forestModel = joblib.load(\"model4.joblib\")\n",
    "forestPrediction = forestModel.predict(xTestScaled)\n",
    "for i in range(len(forestPrediction)): \n",
    "    print(\"Model 6 prediction for item: %d : %s\" % (i + 1, forestPrediction[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forestModel_c = joblib.load(\"model4.joblib\")\n",
    "forestPrediction_c= forestModel_c.predict(xTestScaled)\n",
    "C_answers = yTest\n",
    "TP, FP = 0,0\n",
    "for i in forestPrediction_c: \n",
    "    #print(\"Model 6 prediction for item: %s\" % (forestPrediction_c[i]))\n",
    "    if forestPrediction_c[i] == C_answers[i]: \n",
    "        TP += 1\n",
    "        \n",
    "    elif forestPrediction_c[i] == 1 and C_answers[i]==0 or forestPrediction_c[i]==0 and C_answers[i]==1:\n",
    "        FP += 1\n",
    "print(\"Accuracy Score for Forest Model: %s\" %(sklearn.metrics.accuracy_score(C_answers,forestPrediction_c)))\n",
    "print(\"Precision Score for Forest Model: %s\" %(sklearn.metrics.precision_score(C_answers,forestPrediction_c)))\n",
    "print(\"Recall Score for Forest Model: %s\" % (sklearn.metrics.recall_score(C_answers,forestPrediction_c)))\n",
    "print(\"F1 Score for Forest Model: %s\" %(sklearn.metrics.f1_score(C_answers,forestPrediction_c)))\n",
    "FDR_forest = 1 - sklearn.metrics.precision_score(C_answers,forestPrediction_c)\n",
    "print(\"False Detection Rate for Forest Model: %s\" %(FDR_forest))\n",
    "print(\"Matthews Coefficent for Forest Model: %s\" % (sklearn.metrics.matthews_corrcoef(C_answers,forestPrediction_c)))\n",
    "print(f'=====================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "\n",
    "# Define the model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 21),\n",
    "    'p': [1, 2, 3, 4, 5]  # You may add other values if you want to try custom distance metrics\n",
    "}\n",
    "\n",
    "# Set up Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(xTrainScaled, yTrain)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Save the optimized model\n",
    "joblib.dump(best_model, 'optimized_model2.joblib')\n",
    "\n",
    "# Print the best parameters and accuracy\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validated accuracy:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Train the Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(xTrainScaled, yTrain)\n",
    "\n",
    "# Predict on the training or test set\n",
    "y_pred_continuous = linear_model.predict(xTrainScaled)\n",
    "\n",
    "# Apply threshold rounding to get binary predictions\n",
    "y_pred_binary = np.where(y_pred_continuous >= 0.5, 1, 0)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(yTrain, y_pred_binary)\n",
    "print(\"Accuracy (rounded to binary):\", accuracy)\n",
    "\n",
    "# Save the model if desired\n",
    "joblib.dump(linear_model, 'binary_linear_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xTrain shape: (1913, 1), yTrain shape: (1913,)\n",
      "xTrain shape after reshaping: (1913, 1), yTrain shape after reshaping: (1913,)\n",
      "Test Set Accuracy: 0.6050\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameters for GradientBoostingClassifier\n",
    "n_estimators = 100       # Number of boosting stages (trees)\n",
    "learning_rate = 0.1      # Step size at each iteration\n",
    "max_depth = 3            # Maximum depth of the individual trees\n",
    "subsample = 1.0          # Proportion of samples used to fit each tree\n",
    "min_samples_split = 2    # Minimum number of samples required to split an internal node\n",
    "min_samples_leaf = 1     # Minimum number of samples required to be at a leaf node\n",
    "max_features = None      # Number of features to consider for the best split\n",
    "\n",
    "# Assuming your data is preprocessed into X (features) and y (labels)\n",
    "\n",
    "# Convert Pandas Series to NumPy arrays, if necessary\n",
    "xTrain = xTrain.to_numpy() if isinstance(xTrain, pd.Series) else xTrain\n",
    "yTrain = yTrain.to_numpy() if isinstance(yTrain, pd.Series) else yTrain\n",
    "\n",
    "# Debugging: Check the shape of the input data\n",
    "print(f\"xTrain shape: {xTrain.shape}, yTrain shape: {yTrain.shape}\")\n",
    "\n",
    "# Ensure that X is a 2D array (n_samples, n_features) and y is a 1D array (n_samples,)\n",
    "if len(xTrain.shape) == 1:  # If X has only one feature (1D array)\n",
    "    xTrain = xTrain.reshape(-1, 1)  # Reshape it to be 2D (n_samples, 1)\n",
    "\n",
    "if len(yTrain.shape) > 1:  # If y has more than one column\n",
    "    yTrain = yTrain.reshape(-1, 1)  # Flatten it to be a 1D array\n",
    "\n",
    "# Double-check the shape again after reshaping\n",
    "print(f\"xTrain shape after reshaping: {xTrain.shape}, yTrain shape after reshaping: {yTrain.shape}\")\n",
    "\n",
    "# Define the Gradient Boosting model with the specified parameters\n",
    "model = GradientBoostingClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate=learning_rate,\n",
    "    max_depth=max_depth,\n",
    "    subsample=subsample,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    max_features=max_features\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(xTrain, yTrain)\n",
    "\n",
    "# Convert xTest to numpy if it is a pandas Series, then reshape if it's 1D\n",
    "if isinstance(xTest, pd.Series):\n",
    "    xTest = xTest.to_numpy()\n",
    "\n",
    "# Reshape xTest if it's 1D\n",
    "if len(xTest.shape) == 1:\n",
    "    xTest = xTest.reshape(-1, 1)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "y_pred = model.predict(xTest)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(yTest, y_pred)\n",
    "\n",
    "# Print the final results\n",
    "print(f\"Test Set Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
